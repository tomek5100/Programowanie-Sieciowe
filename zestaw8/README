Zajęcia nr 8, 2022-04-20 i -21
Rozpoczynamy przygotowania do pisania programów, które ściągają dane poprzez HTTP (a także kończycie Państwo pisać sumator w wersji TCP).

Zadania:

Zapoznaj się z co najmniej dwoma-trzema sposobami ściągania zasobów po HTTP. Mogą to być narzędzia linii poleceń wget i curl, biblioteka C/C++ libcurl, standardowe pythonowe moduły urllib i http, niestandardowy requests, klasa java.net.URL i jej metoda openConnection(), itd.

Przez „zapoznaj się” rozumiem „znajdź dokumentację i przeczytaj w niej, co dane narzędzie potrafi zrobić”. Powinno to również obejmować przykłady użycia, tak abyście Państwo mieli orientację czy najprostsze ściągnięcie dokumentu spod określonego URL-a przy pomocy metody GET wymaga napisania trzech linii kodu, czy trzydziestu.

Dla każdego wybranych z narzędzi sprawdź, czy potrafi ono obsługiwać inne metody niż GET i POST, w jaki sposób specyfikuje się argumenty przesyłane w zapytaniach POST, czy potrafi obsługiwać ciasteczka, i czy potrafi je zapisywać w tzw. cookie jar.

Zapoznaj się z pythonową biblioteką Beautiful Soup (lub jej javową wersją znaną jako jsoup). Bardzo ułatwia pisanie programów przetwarzających treść dokumentów HTML.

Zajęcia nr 9, 2022-04-27 i -28
Zadanie:

Napisz program sprawdzający czy pewna określona witryna działa poprawnie. Użyj C/C++, Javy lub Pythona wraz z odpowiednią biblioteką obsługującą wysyłanie zapytań HTTP. Program ma się dawać skompilować i uruchomić na spk-ssh.

Sprawdzenie ma polegać na pobraniu strony spod ustalonego adresu (np. spod http://th.if.uj.edu.pl/). Proszę nie zapomnieć o zweryfikowaniu, czy na pewno udało się ją poprawnie pobrać (status 200) i czy to jest strona HTML (typ text/html). Następnie należy sprawdzić, czy rzeczywiście jest to spodziewana strona, a nie np. komunikat o wewnętrznym błędzie serwera WWW — to można zweryfikować sprawdzając czy w pobranej treści znajduje się pewien zadany z góry ciąg znaków (np. „Institute of Theoretical Physics”).

Program, w zależności od wyniku sprawdzenia, musi zwracać jako wynik funkcji main kod sukcesu (zero) bądź porażki (wartość większa od zera). Osoby piszące w Pythonie powinny użyć sys.exit(0), a w Javie będzie potrzebna instrukcja System.exit(0).

Programy takie jak powyższy są używane w systemach monitorowania usług sieciowych. Jeśli na filmie z centrum zarządzania siecią widać ekran z listą serwerów i usług, a przy nich zielone komunikaty „OK” i gdzieniegdzie czerwone komunikaty błędów, to za tymi kolorami kryją się uruchamiane w regularnych odstępach czasu programy sprawdzające status danej usługi.

Zajęcia nr 10, 2022-05-04 i -05
Kontynuacja poprzednich ćwiczeń, zajmujemy się techniką znaną jako web scraping, czyli zautomatyzowanym pobieraniem danych z witryn internetowych.

Zadanie:

Proszę znaleźć stronę WWW zawierającą jakąś potencjalnie potrzebną informację (aktualna temperatura w Krakowie, kurs dolara, itp.), a następnie napisać program ściągający tę stronę i wyłuskujący z niej te dane. Dane te można zapisywać do jakiegoś pliku lub drukować na stdout, nie jest to ważne — ważne za to jest to, aby format zapisywanych danych pozwalał na ich wygodne dalsze przetwarzanie. Jeśli programowi z jakiejkolwiek przyczyny nie uda się ściągnąć poszukiwanej informacji, to musi zakończyć swe działanie zwracając z main() kod porażki.

Radzę wykorzystać kod programu z poprzednich zajęć — ściąganie już w nim jest, trzeba tylko dodać ekstrahowanie interesujących nas danych.

To zadanie jest ważne — zaimplementowany program prześlij prowadzącemu najpóźniej we wtorek 17 / środę 18 maja.

Wyłuskiwanie danych ze strony HTML jest dość kruchą techniką, bo witryna może nieoczekiwanie (dla nas) zmienić wygląd bądź zawartość. Biorąc to pod uwagę łatwo zauważyć, że podejście typu „zwróć bajty od 5078 do 5081” jest skazane na rychłą porażkę; „zwróć zawartość czwartego elementu <p> znajdującego się wewnątrz elementu <div> o identyfikatorze »temp«” jest lepsze. Warto postarać się o to, aby program zauważał nieoczekiwane bądź podejrzane sytuacje i je zgłaszał (np. jeśli w tym czwartym <p> jest ciąg znaków nie będący liczbą, to raczej nie jest to temperatura; jeśli znaleziona liczba wykracza poza przedział [-30, 40] to raczej nie jest to temperatura w stopniach Celsjusza).

Nawigację po treści strony ułatwia zbudowanie drzewa obiektów reprezentujących elementy strony HTML; ponownie polecam Waszej uwadze bibliotekę Beautiful Soup (Python) i jej odpowiednik JSoup (Java). Do sprawdzania czy łańcuch znaków pasuje do zadanego wzorca dobrze nadają się wyrażenia regularne.

Może się zdarzyć, że traficie na stronę, która wewnątrz przeglądarki wyświetla potrzebne nam informacje, ale gdy się ją ściągnie przez HTTP to nigdzie w jej treści nie można ich znaleźć. Prawie na pewno przyczyną jest jej dynamiczna, AJAX-owa natura. Takie strony mają puste miejsca, wypełniane zawartością ściąganą przez javascriptowy kod z innych URL-i.

Współczesne przeglądarki mają wbudowane narzędzia deweloperskie, jednym z nich jest analizator połączeń sieciowych. Można przy jego pomocy spróbować odnaleźć rzeczywiste źródło potrzebnych nam danych. Poszukiwania proponuję zacząć od połączeń zainicjowanych przez XHR (czyli javascriptową klasę XMLHttpRequest) i / lub tych, które ściągały dokumenty typu JSON.

Udane znalezienie JSON-owego (lub XML-owego) źródła surowych danych jest dobrą wiadomością, bo ryzyko zmiany formatu tych danych jest znacznie mniejsze niż ryzyko zmiany struktury strony HTML.